{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dnRbpbmAJoQ",
    "outputId": "ee356dab-dbb0-4fea-a1e1-badcec8c69d7"
   },
   "source": [
    "# Face Mask Detection – Transfer Learning (DenseNet121)\n",
    "\n",
    "This notebook implements a face mask detection model using **transfer learning** with a pre-trained **DenseNet121** convolutional neural network. It fine-tunes the model on a labeled dataset of face images to classify whether a person is wearing a mask or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# If running in Google Colab, uncomment these lines to mount your Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zggC55zbkz-c"
   },
   "source": [
    "## Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax8Gu-i7pnhc"
   },
   "outputs": [],
   "source": [
    "# update the directories as needed\n",
    "images_directory = os.path.join(\"./drive/My Drive/Colab Notebooks/images\")\n",
    "labels_directory = os.path.join(\"./drive/My Drive/Colab Notebooks/labels\")\n",
    "train_images = os.listdir(images_directory + \"/train\")\n",
    "train_images.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ3lrSJfJZaB",
    "outputId": "816d742a-cff0-4a80-96bd-adcab941411c"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for train_image in tqdm(train_images):\n",
    "  train_label = train_image.split(\".\")[0] + \".txt\"\n",
    "  if train_image != \"subimages\":\n",
    "    f = open(labels_directory + \"/train/\" + train_label, \"r+\")\n",
    "    f_read = (f.read().replace(\"\\n\", \"\")).split(\" \")\n",
    "    if len(f_read) > 5:\n",
    "      continue    # For now, we skip the images with more than one mask\n",
    "    classname = int(f_read[0])\n",
    "    source = images_directory + \"/train/\" + train_image\n",
    "    if classname == 0:\n",
    "      distination = images_directory + \"/train/subimages/mask/\" + train_image\n",
    "      if not os.path.exists(distination):\n",
    "        shutil.copyfile(source, distination)\n",
    "    elif classname == 1:\n",
    "      distination = images_directory + \"/train/subimages/no_mask/\" + train_image\n",
    "      if not os.path.exists(distination):\n",
    "        shutil.copyfile(source, distination)\n",
    "  else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE0xivM6Ruy_"
   },
   "outputs": [],
   "source": [
    "images_directory = os.path.join(\"./drive/My Drive/Colab Notebooks/images\")\n",
    "labels_directory = os.path.join(\"./drive/My Drive/Colab Notebooks/labels\")\n",
    "valid_images = os.listdir(images_directory + \"/valid\")\n",
    "valid_images.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCRiXQzxVqz0",
    "outputId": "78333a3a-c89f-4f2b-cb8a-4dd5bd16e71e"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for valid_image in tqdm(valid_images):\n",
    "  valid_label = valid_image.split(\".\")[0] + \".txt\"\n",
    "  if valid_image != \"subimages\":\n",
    "    f = open(labels_directory + \"/valid/\" + valid_label, \"r+\")\n",
    "    f_read = (f.read().replace(\"\\n\", \"\")).split(\" \")\n",
    "    if len(f_read) > 5:\n",
    "      continue    # For now, we skip the images with more than one mask\n",
    "    classname = int(f_read[0])\n",
    "    source = images_directory + \"/valid/\" + valid_image\n",
    "    if classname == 0:\n",
    "      distination = images_directory + \"/valid/subimages/mask/\" + valid_image\n",
    "      if not os.path.exists(distination):\n",
    "        shutil.copyfile(source, distination)\n",
    "    elif classname == 1:\n",
    "      distination = images_directory + \"/valid/subimages/no_mask/\" + valid_image\n",
    "      if not os.path.exists(distination):\n",
    "        shutil.copyfile(source, distination)\n",
    "  else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrVl02pCwRRZ"
   },
   "outputs": [],
   "source": [
    "images_directory = os.path.join(\"./drive/My Drive/Colab Notebooks/images\")\n",
    "test_images = os.listdir(images_directory + \"/test\")\n",
    "test_images.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9p2TyypR6IC"
   },
   "source": [
    "## Building Data Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No7tpLMj187z"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGG83zZn2JGv"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                       transforms.ToTensor()])\n",
    "valid_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                       transforms.ToTensor()])\n",
    "train_data = datasets.ImageFolder(images_directory + \"/train/subimages\",\n",
    "                                  transform=train_transforms)\n",
    "valid_data = datasets.ImageFolder(images_directory + \"/valid/subimages\",\n",
    "                                  transform=valid_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4quQoXx0xP4P"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                       transforms.ToTensor()])\n",
    "test_data = datasets.ImageFolder(images_directory + \"/test/subimages\",\n",
    "                                  transform=test_transforms)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "438z4qQpZk5D"
   },
   "source": [
    "## Building Transfer Learning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "dc254f864b654f05b73ec56882e35e55",
      "9fbf79d7bdc240be8b41322d24ac534c",
      "dbd2e264e7e447f78f7f6cda89304382",
      "322eea4aac934d83bf9b74157626a947",
      "1ea0599532e643ffa36777dd04e97552",
      "7b84fdd0323847b18c668cf4c4a2796f",
      "73a29a2c9b7640cfba1623e8c08e9f62",
      "cd6419eef995489d8bbb06fee4a8f410"
     ]
    },
    "id": "YEXCRBzp0JP4",
    "outputId": "a59d7ae8-f8ad-4184-b387-095409ad7451"
   },
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.classifier = nn.Sequential(nn.Linear(1024, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss() #negative log likelihood\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzeoaXHiqrF_"
   },
   "source": [
    "## Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJja-y0y0JP4",
    "outputId": "395dcf97-779b-489d-9957-0482343cea32"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for images, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward(images)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            valid_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for images, labels in validloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    logps = model.forward(images)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    valid_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Validation loss: {valid_loss/len(validloader):.3f}.. \"\n",
    "                  f\"Validation accuracy: {accuracy/len(validloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etyGNHtpqy09"
   },
   "source": [
    "## Inference and Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVlHzgWrFT2o"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def view_classify(img, ps):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "    plt.tight_layout()\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(10, 3), ncols=2)\n",
    "    ax1.imshow(img.permute(1, 2, 0))\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(2), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(2))\n",
    "    ax2.set_yticklabels([\"face_mask\", \"face_no_mask\"])\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbdrJB872Mfr"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#images, labels = next(iter(trainloader))\n",
    "for index, (images, labels) in enumerate(trainloader):\n",
    "  if index == 2:\n",
    "    break\n",
    "# img = images[0].view(1, 2500)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model.forward(images)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "SYfGSRo0pOB0",
    "outputId": "5142dd75-51c3-49ab-97d7-f1369f0d7735"
   },
   "outputs": [],
   "source": [
    "print(\"Example Training Image\")\n",
    "fig = view_classify(images[2], ps[2])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwLH9_KIzDJB"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#images, labels = next(iter(trainloader))\n",
    "for index, (images, labels) in enumerate(testloader):\n",
    "  if index == 2:\n",
    "    break\n",
    "# img = images[0].view(1, 2500)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model.forward(images)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "DZS70fMYzZqY",
    "outputId": "320854fc-db54-48e9-8f33-c917e52929dc"
   },
   "outputs": [],
   "source": [
    "print(\"Example Test Image\")\n",
    "fig = view_classify(images[0], ps[0])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn7-9tMCq5n0"
   },
   "source": [
    "## Plotting the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AWIc6__sRcW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(true_values, prediction_values):\n",
    "  fig, ax = plt.subplots()\n",
    "  cm = confusion_matrix(true_values, prediction_values)\n",
    "  tn, fp, fn, tp = cm.ravel()\n",
    "  print(\"TN:\",tn, \" FP:\", fp, \" FN:\", fn, \" TP:\", tp)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Mask\", \"No Mask\"])\n",
    "  disp.plot(cmap='GnBu', ax=ax)\n",
    "  ax.set_title(\"densenet121\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddSiUnlDscpM"
   },
   "source": [
    "Let us first plot the confusion matrix on the train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRT8_NDCql0k",
    "outputId": "93016dfa-67bd-4187-b64f-876af9ad21b6"
   },
   "outputs": [],
   "source": [
    "y_train = []\n",
    "pred_train = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for images, labels in trainloader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    logps = model.forward(images)\n",
    "    ps = torch.exp(logps)\n",
    "    for i, p in enumerate(ps.numpy()):\n",
    "      y_train.append(labels[i])\n",
    "      pred_train.append(p.argmax())\n",
    "print(\"len(y_train) =\", len(y_train))\n",
    "print(\"len(pred_train) =\", len(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "Ye1JIJA9s9sy",
    "outputId": "e0412d9f-1db4-4977-8b4d-35ac4ff5f167"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_values=y_train, prediction_values=pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAglBJQotFWY"
   },
   "source": [
    "Plot for the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BA0iYBrItOaC",
    "outputId": "ad075cd4-d1cf-4ff8-960c-22a1b6847206"
   },
   "outputs": [],
   "source": [
    "y_valid = []\n",
    "pred_valid = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for images, labels in validloader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    logps = model.forward(images)\n",
    "    ps = torch.exp(logps)\n",
    "    for i, p in enumerate(ps.numpy()):\n",
    "      y_valid.append(labels[i])\n",
    "      pred_valid.append(p.argmax())\n",
    "print(\"len(y_valid) =\", len(y_valid))\n",
    "print(\"len(pred_valid) =\", len(pred_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "uylCPAAwtgD9",
    "outputId": "5550d95e-cf22-4bdc-8d8c-53a6fb639609"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true_values=y_valid, prediction_values=pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOSkg2q0reQB",
    "outputId": "449fac96-df1d-4f79-d7c3-f64329672a2d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_valid, pred_valid))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Myphoto_Mask_Pytorch_Transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ea0599532e643ffa36777dd04e97552": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "322eea4aac934d83bf9b74157626a947": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd6419eef995489d8bbb06fee4a8f410",
      "placeholder": "​",
      "style": "IPY_MODEL_73a29a2c9b7640cfba1623e8c08e9f62",
      "value": " 30.8M/30.8M [00:00&lt;00:00, 70.5MB/s]"
     }
    },
    "73a29a2c9b7640cfba1623e8c08e9f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b84fdd0323847b18c668cf4c4a2796f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fbf79d7bdc240be8b41322d24ac534c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd6419eef995489d8bbb06fee4a8f410": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbd2e264e7e447f78f7f6cda89304382": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b84fdd0323847b18c668cf4c4a2796f",
      "max": 32342954,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ea0599532e643ffa36777dd04e97552",
      "value": 32342954
     }
    },
    "dc254f864b654f05b73ec56882e35e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbd2e264e7e447f78f7f6cda89304382",
       "IPY_MODEL_322eea4aac934d83bf9b74157626a947"
      ],
      "layout": "IPY_MODEL_9fbf79d7bdc240be8b41322d24ac534c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
